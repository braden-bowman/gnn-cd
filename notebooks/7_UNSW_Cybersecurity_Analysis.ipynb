{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cybersecurity Analysis with Community Detection\n",
    "\n",
    "In this notebook, we'll apply our community detection methods to cybersecurity data from the UNSW-NB15 dataset. We'll:\n",
    "\n",
    "1. Process the UNSW-NB15 dataset and construct a graph\n",
    "2. Perform feature selection to identify important network attributes\n",
    "3. Apply various community detection methods\n",
    "4. Evaluate how well communities align with attack patterns\n",
    "5. Compare the performance of different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T21:34:41.897491Z",
     "iopub.status.busy": "2025-03-31T21:34:41.897258Z",
     "iopub.status.idle": "2025-03-31T21:34:53.420370Z",
     "shell.execute_reply": "2025-03-31T21:34:53.419220Z",
     "shell.execute_reply.started": "2025-03-31T21:34:41.897476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to be able to use all crisp methods, you need to install some additional packages:  {'graph_tool'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\t\t\t<script type=\"text/javascript\">\n",
       "\t\t\t<!--\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_script');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('script');\n",
       "\t\t\t\telement.type = 'text/javascript';\n",
       "\t\t\t\telement.innerHTML = 'function NetworKit_pageEmbed(id) { var i, j; var elements; elements = document.getElementById(id).getElementsByClassName(\"Plot\"); for (i=0; i<elements.length; i++) { elements[i].id = id + \"_Plot_\" + i; var data = elements[i].getAttribute(\"data-image\").split(\"|\"); elements[i].removeAttribute(\"data-image\"); var content = \"<div class=\\\\\"Image\\\\\" id=\\\\\"\" + elements[i].id + \"_Image\\\\\" />\"; elements[i].innerHTML = content; elements[i].setAttribute(\"data-image-index\", 0); elements[i].setAttribute(\"data-image-length\", data.length); for (j=0; j<data.length; j++) { elements[i].setAttribute(\"data-image-\" + j, data[j]); } NetworKit_plotUpdate(elements[i]); elements[i].onclick = function (e) { NetworKit_overlayShow((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"HeatCell\"); for (i=0; i<elements.length; i++) { var data = parseFloat(elements[i].getAttribute(\"data-heat\")); var color = \"#00FF00\"; if (data <= 1 && data > 0) { color = \"hsla(0, 100%, 75%, \" + (data) + \")\"; } else if (data <= 0 && data >= -1) { color = \"hsla(240, 100%, 75%, \" + (-data) + \")\"; } elements[i].style.backgroundColor = color; } elements = document.getElementById(id).getElementsByClassName(\"Details\"); for (i=0; i<elements.length; i++) { elements[i].setAttribute(\"data-title\", \"-\"); NetworKit_toggleDetails(elements[i]); elements[i].onclick = function (e) { NetworKit_toggleDetails((e.target) ? e.target : e.srcElement); } } elements = document.getElementById(id).getElementsByClassName(\"MathValue\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"nan\") { elements[i].parentNode.innerHTML = \"\" } } elements = document.getElementById(id).getElementsByClassName(\"SubCategory\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } elements = document.getElementById(id).getElementsByClassName(\"Category\"); for (i=elements.length-1; i>=0; i--) { value = elements[i].innerHTML.trim(); if (value === \"\") { elements[i].parentNode.removeChild(elements[i]) } } var isFirefox = false; try { isFirefox = typeof InstallTrigger !== \"undefined\"; } catch (e) {} if (!isFirefox) { alert(\"Currently the function\\'s output is only fully supported by Firefox.\"); } } function NetworKit_plotUpdate(source) { var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(source.id + \"_Image\"); image.style.backgroundImage = \"url(\" + data + \")\"; } function NetworKit_showElement(id, show) { var element = document.getElementById(id); element.style.display = (show) ? \"block\" : \"none\"; } function NetworKit_overlayShow(source) { NetworKit_overlayUpdate(source); NetworKit_showElement(\"NetworKit_Overlay\", true); } function NetworKit_overlayUpdate(source) { document.getElementById(\"NetworKit_Overlay_Title\").innerHTML = source.title; var index = source.getAttribute(\"data-image-index\"); var data = source.getAttribute(\"data-image-\" + index); var image = document.getElementById(\"NetworKit_Overlay_Image\"); image.setAttribute(\"data-id\", source.id); image.style.backgroundImage = \"url(\" + data + \")\"; var link = document.getElementById(\"NetworKit_Overlay_Toolbar_Bottom_Save\"); link.href = data; link.download = source.title + \".svg\"; } function NetworKit_overlayImageShift(delta) { var image = document.getElementById(\"NetworKit_Overlay_Image\"); var source = document.getElementById(image.getAttribute(\"data-id\")); var index = parseInt(source.getAttribute(\"data-image-index\")); var length = parseInt(source.getAttribute(\"data-image-length\")); var index = (index+delta) % length; if (index < 0) { index = length + index; } source.setAttribute(\"data-image-index\", index); NetworKit_overlayUpdate(source); } function NetworKit_toggleDetails(source) { var childs = source.children; var show = false; if (source.getAttribute(\"data-title\") == \"-\") { source.setAttribute(\"data-title\", \"+\"); show = false; } else { source.setAttribute(\"data-title\", \"-\"); show = true; } for (i=0; i<childs.length; i++) { if (show) { childs[i].style.display = \"block\"; } else { childs[i].style.display = \"none\"; } } }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_script');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_style');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('style');\n",
       "\t\t\t\telement.type = 'text/css';\n",
       "\t\t\t\telement.innerHTML = '.NetworKit_Page { font-family: Arial, Helvetica, sans-serif; font-size: 14px; } .NetworKit_Page .Value:before { font-family: Arial, Helvetica, sans-serif; font-size: 1.05em; content: attr(data-title) \":\"; margin-left: -2.5em; padding-right: 0.5em; } .NetworKit_Page .Details .Value:before { display: block; } .NetworKit_Page .Value { font-family: monospace; white-space: pre; padding-left: 2.5em; white-space: -moz-pre-wrap !important; white-space: -pre-wrap; white-space: -o-pre-wrap; white-space: pre-wrap; word-wrap: break-word; tab-size: 4; -moz-tab-size: 4; } .NetworKit_Page .Category { clear: both; padding-left: 1em; margin-bottom: 1.5em; } .NetworKit_Page .Category:before { content: attr(data-title); font-size: 1.75em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory { margin-bottom: 1.5em; padding-left: 1em; } .NetworKit_Page .SubCategory:before { font-size: 1.6em; display: block; margin-left: -0.8em; margin-bottom: 0.5em; } .NetworKit_Page .SubCategory[data-title]:before { content: attr(data-title); } .NetworKit_Page .Block { display: block; } .NetworKit_Page .Block:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .Block .Thumbnail_Overview, .NetworKit_Page .Block .Thumbnail_ScatterPlot { width: 260px; float: left; } .NetworKit_Page .Block .Thumbnail_Overview img, .NetworKit_Page .Block .Thumbnail_ScatterPlot img { width: 260px; } .NetworKit_Page .Block .Thumbnail_Overview:before, .NetworKit_Page .Block .Thumbnail_ScatterPlot:before { display: block; text-align: center; font-weight: bold; } .NetworKit_Page .Block .Thumbnail_Overview:before { content: attr(data-title); } .NetworKit_Page .HeatCell { font-family: \"Courier New\", Courier, monospace; cursor: pointer; } .NetworKit_Page .HeatCell, .NetworKit_Page .HeatCellName { display: inline; padding: 0.1em; margin-right: 2px; background-color: #FFFFFF } .NetworKit_Page .HeatCellName { margin-left: 0.25em; } .NetworKit_Page .HeatCell:before { content: attr(data-heat); display: inline-block; color: #000000; width: 4em; text-align: center; } .NetworKit_Page .Measure { clear: both; } .NetworKit_Page .Measure .Details { cursor: pointer; } .NetworKit_Page .Measure .Details:before { content: \"[\" attr(data-title) \"]\"; display: block; } .NetworKit_Page .Measure .Details .Value { border-left: 1px dotted black; margin-left: 0.4em; padding-left: 3.5em; pointer-events: none; } .NetworKit_Page .Measure .Details .Spacer:before { content: \".\"; opacity: 0.0; pointer-events: none; } .NetworKit_Page .Measure .Plot { width: 440px; height: 440px; cursor: pointer; float: left; margin-left: -0.9em; margin-right: 20px; } .NetworKit_Page .Measure .Plot .Image { background-repeat: no-repeat; background-position: center center; background-size: contain; height: 100%; pointer-events: none; } .NetworKit_Page .Measure .Stat { width: 500px; float: left; } .NetworKit_Page .Measure .Stat .Group { padding-left: 1.25em; margin-bottom: 0.75em; } .NetworKit_Page .Measure .Stat .Group .Title { font-size: 1.1em; display: block; margin-bottom: 0.3em; margin-left: -0.75em; border-right-style: dotted; border-right-width: 1px; border-bottom-style: dotted; border-bottom-width: 1px; background-color: #D0D0D0; padding-left: 0.2em; } .NetworKit_Page .Measure .Stat .Group .List { -webkit-column-count: 3; -moz-column-count: 3; column-count: 3; } .NetworKit_Page .Measure .Stat .Group .List .Entry { position: relative; line-height: 1.75em; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:before { position: absolute; left: 0; top: -40px; background-color: #808080; color: #ffffff; height: 30px; line-height: 30px; border-radius: 5px; padding: 0 15px; content: attr(data-tooltip); white-space: nowrap; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:after { position: absolute; left: 15px; top: -10px; border-top: 7px solid #808080; border-left: 7px solid transparent; border-right: 7px solid transparent; content: \"\"; display: none; } .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:after, .NetworKit_Page .Measure .Stat .Group .List .Entry[data-tooltip]:hover:before { display: block; } .NetworKit_Page .Measure .Stat .Group .List .Entry .MathValue { font-family: \"Courier New\", Courier, monospace; } .NetworKit_Page .Measure:after { content: \".\"; visibility: hidden; display: block; height: 0; clear: both; } .NetworKit_Page .PartitionPie { clear: both; } .NetworKit_Page .PartitionPie img { width: 600px; } #NetworKit_Overlay { left: 0px; top: 0px; display: none; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.6); z-index: 1000; } #NetworKit_Overlay_Title { position: absolute; color: white; transform: rotate(-90deg); width: 32em; height: 32em; padding-right: 0.5em; padding-top: 0.5em; text-align: right; font-size: 40px; } #NetworKit_Overlay .button { background: white; cursor: pointer; } #NetworKit_Overlay .button:before { size: 13px; display: inline-block; text-align: center; margin-top: 0.5em; margin-bottom: 0.5em; width: 1.5em; height: 1.5em; } #NetworKit_Overlay .icon-close:before { content: \"X\"; } #NetworKit_Overlay .icon-previous:before { content: \"P\"; } #NetworKit_Overlay .icon-next:before { content: \"N\"; } #NetworKit_Overlay .icon-save:before { content: \"S\"; } #NetworKit_Overlay_Toolbar_Top, #NetworKit_Overlay_Toolbar_Bottom { position: absolute; width: 40px; right: 13px; text-align: right; z-index: 1100; } #NetworKit_Overlay_Toolbar_Top { top: 0.5em; } #NetworKit_Overlay_Toolbar_Bottom { Bottom: 0.5em; } #NetworKit_Overlay_ImageContainer { position: absolute; top: 5%; left: 5%; height: 90%; width: 90%; background-repeat: no-repeat; background-position: center center; background-size: contain; } #NetworKit_Overlay_Image { height: 100%; width: 100%; background-repeat: no-repeat; background-position: center center; background-size: contain; }';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_style');\n",
       "\t\t\t\tdocument.head.appendChild(element);\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t\t\n",
       "\t\t\t{\n",
       "\t\t\t\tvar element = document.getElementById('NetworKit_Overlay');\n",
       "\t\t\t\tif (element) {\n",
       "\t\t\t\t\telement.parentNode.removeChild(element);\n",
       "\t\t\t\t}\n",
       "\t\t\t\telement = document.createElement('div');\n",
       "\t\t\t\telement.innerHTML = '<div id=\"NetworKit_Overlay_Toolbar_Top\"><div class=\"button icon-close\" id=\"NetworKit_Overlay_Close\" /></div><div id=\"NetworKit_Overlay_Title\" /> <div id=\"NetworKit_Overlay_ImageContainer\"> <div id=\"NetworKit_Overlay_Image\" /> </div> <div id=\"NetworKit_Overlay_Toolbar_Bottom\"> <div class=\"button icon-previous\" onclick=\"NetworKit_overlayImageShift(-1)\" /> <div class=\"button icon-next\" onclick=\"NetworKit_overlayImageShift(1)\" /> <a id=\"NetworKit_Overlay_Toolbar_Bottom_Save\"><div class=\"button icon-save\" /></a> </div>';\n",
       "\t\t\t\telement.setAttribute('id', 'NetworKit_Overlay');\n",
       "\t\t\t\tdocument.body.appendChild(element);\n",
       "\t\t\t\tdocument.getElementById('NetworKit_Overlay_Close').onclick = function (e) {\n",
       "\t\t\t\t\tdocument.getElementById('NetworKit_Overlay').style.display = 'none';\n",
       "\t\t\t\t}\n",
       "\t\t\t}\n",
       "\t\t\n",
       "\t\t\t-->\n",
       "\t\t\t</script>\n",
       "\t\t"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 17:34:48.390494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743456888.463590  630887 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743456888.483185  630887 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743456890.894644  630887 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743456890.894694  630887 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743456890.894696  630887 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743456890.894697  630887 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-31 17:34:50.915355: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'run_gcn' from 'community_detection.gnn_community_detection' (/home/braden/gnn-cd/community_detection/gnn_community_detection.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Import community detection methods\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommunity_detection\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtraditional_methods\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     run_louvain, run_leiden, run_infomap, run_label_propagation, run_spectral_clustering\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommunity_detection\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgnn_community_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     23\u001b[39m     run_gcn, run_graphsage, run_gat, run_vgae\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommunity_detection\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moverlapping_community_detection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     26\u001b[39m     run_bigclam, run_demon, run_slpa, run_gnn_overlapping\n\u001b[32m     27\u001b[39m )\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommunity_detection\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvisualization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m visualize_communities\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'run_gcn' from 'community_detection.gnn_community_detection' (/home/braden/gnn-cd/community_detection/gnn_community_detection.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import rustworkx as rx\n",
    "import torch\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import community detection methods\n",
    "from community_detection.traditional_methods import (\n",
    "    run_louvain, run_leiden, run_infomap, run_label_propagation, run_spectral_clustering\n",
    ")\n",
    "from community_detection.gnn_community_detection import (\n",
    "    run_gcn, run_graphsage, run_gat, run_vgae\n",
    ")\n",
    "from community_detection.overlapping_community_detection import (\n",
    "    run_bigclam, run_demon, run_slpa, run_gnn_overlapping\n",
    ")\n",
    "from community_detection.visualization import visualize_communities\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = os.path.join(os.getcwd(), '..', 'data', 'unsw')\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "UNSW_FEATURES_PATH = os.path.join(DATA_DIR, \"UNSW-NB15_features.csv\")\n",
    "UNSW_DATA_PATH_1 = os.path.join(DATA_DIR, \"UNSW-NB15_1.csv\")\n",
    "GRAPH_PATH = os.path.join(DATA_DIR, \"unsw_graph.pt\")\n",
    "RESULTS_DIR = os.path.join(DATA_DIR, \"results\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and Process the UNSW-NB15 Dataset\n",
    "\n",
    "The UNSW-NB15 dataset is a comprehensive network traffic dataset that contains normal traffic and attack traffic. It includes 49 features and labels for different attack types.\n",
    "\n",
    "Note: You'll need to manually download the dataset from https://research.unsw.edu.au/projects/unsw-nb15-dataset and place the CSV files in the data/unsw directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_paths, features_path):\n",
    "    \"\"\"\n",
    "    Load the UNSW-NB15 dataset and its features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_paths: list\n",
    "        List of paths to CSV data files\n",
    "    features_path: str\n",
    "        Path to features CSV file\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    data: pandas.DataFrame\n",
    "        Combined dataset\n",
    "    features_info: pandas.DataFrame\n",
    "        Feature information\n",
    "    \"\"\"\n",
    "    print(f\"Loading feature information from {features_path}\")\n",
    "    try:\n",
    "        # Load feature information\n",
    "        features_info = pd.read_csv(features_path)\n",
    "        print(f\"Loaded {len(features_info)} features\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Features file not found: {features_path}\")\n",
    "        features_info = None\n",
    "    \n",
    "    # Check if any data files exist\n",
    "    existing_paths = [path for path in data_paths if os.path.exists(path)]\n",
    "    if not existing_paths:\n",
    "        print(\"No data files found. Please download the UNSW-NB15 dataset first.\")\n",
    "        print(\"Download from: https://research.unsw.edu.au/projects/unsw-nb15-dataset\")\n",
    "        return None, features_info\n",
    "    \n",
    "    # Load and combine data files\n",
    "    dfs = []\n",
    "    for path in existing_paths:\n",
    "        print(f\"Loading data from {path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            dfs.append(df)\n",
    "            print(f\"Loaded {len(df)} records from {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "    \n",
    "    if not dfs:\n",
    "        return None, features_info\n",
    "    \n",
    "    data = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Combined dataset has {len(data)} records\")\n",
    "    return data, features_info\n",
    "\n",
    "# Load the dataset\n",
    "data_paths = [UNSW_DATA_PATH_1]\n",
    "data, features_info = load_dataset(data_paths, UNSW_FEATURES_PATH)\n",
    "\n",
    "# If no real data is available, create synthetic data for demonstration\n",
    "if data is None:\n",
    "    print(\"Creating synthetic data for demonstration...\")\n",
    "    # Create synthetic data with similar structure to UNSW-NB15\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    n_features = 10\n",
    "    \n",
    "    # Generate synthetic features\n",
    "    X = np.random.rand(n_samples, n_features)\n",
    "    \n",
    "    # Generate synthetic source and destination IPs\n",
    "    src_ips = [f\"192.168.1.{np.random.randint(1, 255)}\" for _ in range(n_samples)]\n",
    "    dst_ips = [f\"10.0.0.{np.random.randint(1, 255)}\" for _ in range(n_samples)]\n",
    "    \n",
    "    # Generate synthetic labels (20% attacks)\n",
    "    labels = np.random.choice([0, 1], size=n_samples, p=[0.8, 0.2])\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = [f\"feature_{i}\" for i in range(n_features)]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame(X, columns=feature_names)\n",
    "    data['srcip'] = src_ips\n",
    "    data['dstip'] = dst_ips\n",
    "    data['label'] = labels\n",
    "    \n",
    "    # Create some correlation between features and labels\n",
    "    for i in range(3):\n",
    "        data.loc[data['label'] == 1, f'feature_{i}'] += 0.3\n",
    "    \n",
    "    print(f\"Created synthetic dataset with {len(data)} records\")\n",
    "\n",
    "# Display the first few rows\n",
    "if data is not None:\n",
    "    print(\"\\nDataset preview:\")\n",
    "    display(data.head())\n",
    "    \n",
    "    # Display class distribution\n",
    "    if 'label' in data.columns:\n",
    "        attack_count = data['label'].sum()\n",
    "        normal_count = len(data) - attack_count\n",
    "        print(f\"\\nClass distribution:\\n- Normal: {normal_count} ({normal_count/len(data)*100:.1f}%)\\n- Attack: {attack_count} ({attack_count/len(data)*100:.1f}%)\")\n",
    "        \n",
    "        # Plot class distribution\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.countplot(x='label', data=data)\n",
    "        plt.title('Class Distribution')\n",
    "        plt.xlabel('Label (0=Normal, 1=Attack)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection and Analysis\n",
    "\n",
    "Let's perform feature selection to identify the most important features for attack detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(data, k=20):\n",
    "    \"\"\"\n",
    "    Perform feature selection on the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: pandas.DataFrame\n",
    "        Dataset with features and labels\n",
    "    k: int\n",
    "        Number of features to select\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    selected_features: list\n",
    "        List of selected feature names\n",
    "    X_selected: numpy.ndarray\n",
    "        Feature matrix with selected features\n",
    "    y: numpy.ndarray\n",
    "        Labels array\n",
    "    \"\"\"\n",
    "    # Separate features and labels\n",
    "    exclude_cols = ['label', 'attack_cat', 'srcip', 'dstip']\n",
    "    X = data.drop([col for col in exclude_cols if col in data.columns], axis=1)\n",
    "    y = data['label'] if 'label' in data.columns else np.zeros(len(data))\n",
    "    \n",
    "    # Handle categorical features\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Apply feature selection\n",
    "    selector = SelectKBest(f_classif, k=min(k, X.shape[1]))\n",
    "    X_selected = selector.fit_transform(X_scaled, y)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = X.columns[selector.get_support()].tolist()\n",
    "    print(f\"Selected {len(selected_features)} features:\\n{', '.join(selected_features)}\")\n",
    "    \n",
    "    # Visualize feature importance scores\n",
    "    scores = selector.scores_\n",
    "    feature_scores = list(zip(X.columns, scores))\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_features = feature_scores[:k]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh([f[0] for f in top_features], [f[1] for f in top_features])\n",
    "    plt.xlabel('F-Score')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Top {k} Features by F-Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_features, X_selected, y\n",
    "\n",
    "# Perform feature selection\n",
    "if data is not None:\n",
    "    selected_features, X_selected, y = feature_selection(data, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Construct Network Graph\n",
    "\n",
    "Now we'll construct a graph from the dataset where nodes represent devices (IPs) and edges represent connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(data, feature_cols, target_col='label'):\n",
    "    \"\"\"\n",
    "    Construct a graph from the dataset where devices are nodes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: pandas.DataFrame\n",
    "        Dataset with features and labels\n",
    "    feature_cols: list\n",
    "        List of feature columns to use\n",
    "    target_col: str\n",
    "        Target column name\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    G: rustworkx.PyGraph\n",
    "        Constructed graph\n",
    "    node_mapping: dict\n",
    "        Mapping from node IDs to device IPs\n",
    "    \"\"\"\n",
    "    # Extract unique source and destination IPs\n",
    "    src_ips = set(data['srcip'].unique()) if 'srcip' in data.columns else set()\n",
    "    dst_ips = set(data['dstip'].unique()) if 'dstip' in data.columns else set()\n",
    "    all_ips = src_ips.union(dst_ips)\n",
    "    \n",
    "    if not all_ips:\n",
    "        # If IP columns not found, use a different approach\n",
    "        print(\"IP columns not found. Using synthetic node IDs based on row indices.\")\n",
    "        all_ips = set(range(len(data)))\n",
    "        ip_to_idx = {i: i for i in all_ips}\n",
    "        data['srcip'] = data.index\n",
    "        data['dstip'] = (data.index + 1) % len(data)  # Create connections in a ring\n",
    "    else:\n",
    "        # Create mapping from IPs to node indices\n",
    "        ip_to_idx = {ip: i for i, ip in enumerate(all_ips)}\n",
    "    \n",
    "    # Create graph\n",
    "    G = rx.PyGraph()\n",
    "    \n",
    "    # Add nodes with feature vectors and labels\n",
    "    node_mapping = {}\n",
    "    for ip in all_ips:\n",
    "        # Get rows where this IP appears as source or destination\n",
    "        ip_data = data[(data['srcip'] == ip) | (data['dstip'] == ip)]\n",
    "        \n",
    "        if len(ip_data) == 0:\n",
    "            # If no data for this IP, use zeros for features\n",
    "            features = np.zeros(len(feature_cols))\n",
    "            label = 0\n",
    "        else:\n",
    "            # For demonstration with synthetic data, handle missing columns\n",
    "            available_features = [f for f in feature_cols if f in ip_data.columns]\n",
    "            \n",
    "            if not available_features:\n",
    "                # If no selected features are available, use random features\n",
    "                features = np.random.rand(len(feature_cols))\n",
    "            else:\n",
    "                # Aggregate features for this IP (using mean)\n",
    "                features = ip_data[available_features].mean().values\n",
    "                # Pad with zeros if needed\n",
    "                if len(features) < len(feature_cols):\n",
    "                    features = np.pad(features, (0, len(feature_cols) - len(features)))\n",
    "            \n",
    "            # Determine label (1 if any traffic involving this IP is malicious)\n",
    "            label = 1 if target_col in ip_data.columns and (ip_data[target_col] == 1).any() else 0\n",
    "        \n",
    "        # Create node data with features and label\n",
    "        node_data = {\n",
    "            'features': features,\n",
    "            'label': label,\n",
    "            'ip': ip\n",
    "        }\n",
    "        \n",
    "        # Add node to graph\n",
    "        node_idx = G.add_node(node_data)\n",
    "        node_mapping[ip] = node_idx\n",
    "    \n",
    "    # Add edges based on traffic between IPs\n",
    "    edge_counts = {}\n",
    "    for _, row in data.iterrows():\n",
    "        src = row['srcip']\n",
    "        dst = row['dstip']\n",
    "        if src != dst:  # Avoid self-loops\n",
    "            src_idx = node_mapping[src]\n",
    "            dst_idx = node_mapping[dst]\n",
    "            \n",
    "            # Count occurrences of this edge for weight\n",
    "            edge_key = (src_idx, dst_idx)\n",
    "            edge_counts[edge_key] = edge_counts.get(edge_key, 0) + 1\n",
    "    \n",
    "    # Add weighted edges to graph\n",
    "    for (src_idx, dst_idx), weight in edge_counts.items():\n",
    "        G.add_edge(src_idx, dst_idx, weight)\n",
    "    \n",
    "    print(f\"Created graph with {len(G)} nodes and {G.num_edges()} edges\")\n",
    "    return G, node_mapping\n",
    "\n",
    "def visualize_graph(G, node_mapping, max_nodes=100):\n",
    "    \"\"\"\n",
    "    Visualize the graph\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G: rustworkx.PyGraph\n",
    "        Graph to visualize\n",
    "    node_mapping: dict\n",
    "        Mapping from device IPs to node IDs\n",
    "    max_nodes: int\n",
    "        Maximum number of nodes to show\n",
    "    \"\"\"\n",
    "    # Convert to NetworkX for visualization\n",
    "    G_nx = nx.Graph()\n",
    "    \n",
    "    # Limit to max_nodes if graph is too large\n",
    "    nodes_to_show = min(len(G), max_nodes)\n",
    "    print(f\"Showing {nodes_to_show} out of {len(G)} nodes\")\n",
    "    \n",
    "    # Add nodes with attributes\n",
    "    for i in range(nodes_to_show):\n",
    "        node_data = G.get_node_data(i)\n",
    "        label = node_data['label']\n",
    "        G_nx.add_node(i, label=label)\n",
    "    \n",
    "    # Add edges\n",
    "    for edge in G.edge_list():\n",
    "        source, target = edge[0], edge[1]\n",
    "        if source < nodes_to_show and target < nodes_to_show:\n",
    "            weight = G.get_edge_data(source, target)\n",
    "            G_nx.add_edge(source, target, weight=weight)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = nx.spring_layout(G_nx, seed=42)\n",
    "    \n",
    "    # Get node colors based on labels\n",
    "    node_colors = [G_nx.nodes[n]['label'] for n in G_nx.nodes()]\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G_nx, pos, \n",
    "                          node_color=node_colors, \n",
    "                          cmap=plt.cm.coolwarm, \n",
    "                          alpha=0.8, \n",
    "                          node_size=100)\n",
    "    \n",
    "    # Draw edges with width based on weight\n",
    "    edge_widths = [G_nx[u][v].get('weight', 1) / 10 for u, v in G_nx.edges()]\n",
    "    nx.draw_networkx_edges(G_nx, pos, width=edge_widths, alpha=0.3)\n",
    "    \n",
    "    plt.title(f\"Network Graph from UNSW-NB15 (showing {nodes_to_show} nodes)\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.coolwarm, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "    sm.set_array([])\n",
    "    plt.colorbar(sm, label='Attack Label (1=Attack)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(DATA_DIR, 'unsw_graph.png'))\n",
    "    plt.show()\n",
    "\n",
    "def save_graph(G, filepath):\n",
    "    \"\"\"\n",
    "    Save the graph to a file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G: rustworkx.PyGraph\n",
    "        Graph to save\n",
    "    filepath: str\n",
    "        Path to save the graph\n",
    "    \"\"\"\n",
    "    # Convert node data to serializable format\n",
    "    for i in range(len(G)):\n",
    "        node_data = G.get_node_data(i)\n",
    "        if node_data:\n",
    "            # Convert numpy arrays to lists\n",
    "            if 'features' in node_data and isinstance(node_data['features'], np.ndarray):\n",
    "                node_data['features'] = node_data['features'].tolist()\n",
    "            G.set_node_data(i, node_data)\n",
    "    \n",
    "    # Save using torch\n",
    "    data = {\n",
    "        'num_nodes': len(G),\n",
    "        'edge_list': G.edge_list(),\n",
    "        'node_data': [G.get_node_data(i) for i in range(len(G))],\n",
    "        'edge_data': [G.get_edge_data(e[0], e[1]) for e in G.edge_list()]\n",
    "    }\n",
    "    torch.save(data, filepath)\n",
    "    print(f\"Graph saved to {filepath}\")\n",
    "\n",
    "def load_graph(filepath):\n",
    "    \"\"\"\n",
    "    Load a graph from a file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath: str\n",
    "        Path to the saved graph\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    G: rustworkx.PyGraph\n",
    "        Loaded graph\n",
    "    \"\"\"\n",
    "    data = torch.load(filepath)\n",
    "    G = rx.PyGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for node_data in data['node_data']:\n",
    "        # Convert features back to numpy array if needed\n",
    "        if 'features' in node_data and isinstance(node_data['features'], list):\n",
    "            node_data['features'] = np.array(node_data['features'])\n",
    "        G.add_node(node_data)\n",
    "    \n",
    "    # Add edges\n",
    "    for (src, dst), edge_data in zip(data['edge_list'], data['edge_data']):\n",
    "        G.add_edge(src, dst, edge_data)\n",
    "    \n",
    "    print(f\"Loaded graph with {len(G)} nodes and {G.num_edges()} edges\")\n",
    "    return G\n",
    "\n",
    "# Construct and visualize the graph\n",
    "if data is not None and 'selected_features' in locals():\n",
    "    # Check if graph already exists\n",
    "    if os.path.exists(GRAPH_PATH):\n",
    "        print(f\"Loading existing graph from {GRAPH_PATH}\")\n",
    "        G = load_graph(GRAPH_PATH)\n",
    "        # Dummy node mapping for existing graph\n",
    "        node_mapping = {i: i for i in range(len(G))}\n",
    "    else:\n",
    "        print(\"Constructing graph from data...\")\n",
    "        G, node_mapping = construct_graph(data, selected_features)\n",
    "        save_graph(G, GRAPH_PATH)\n",
    "    \n",
    "    # Visualize graph\n",
    "    visualize_graph(G, node_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Community Detection Methods\n",
    "\n",
    "Now we'll apply various community detection methods and evaluate their performance for identifying attack-related communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_communities(G, communities, community_type='non-overlapping'):\n",
    "    \"\"\"\n",
    "    Evaluate community detection results for cybersecurity\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G: rustworkx.PyGraph\n",
    "        Graph with ground truth labels\n",
    "    communities: dict or list\n",
    "        Detected communities (dict for non-overlapping, list of lists for overlapping)\n",
    "    community_type: str\n",
    "        'non-overlapping' or 'overlapping'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics: dict\n",
    "        Evaluation metrics\n",
    "    \"\"\"\n",
    "    if community_type == 'non-overlapping':\n",
    "        # Convert community dict to community assignments\n",
    "        community_assignments = {}\n",
    "        for node, community in communities.items():\n",
    "            if community not in community_assignments:\n",
    "                community_assignments[community] = []\n",
    "            community_assignments[community].append(node)\n",
    "        \n",
    "        communities_list = list(community_assignments.values())\n",
    "    else:\n",
    "        # Already in list of lists format\n",
    "        communities_list = communities\n",
    "    \n",
    "    # Calculate homogeneity of communities regarding attack labels\n",
    "    y_true = np.array([G.get_node_data(i)['label'] for i in range(len(G))])\n",
    "    \n",
    "    # Assign predicted label to each node based on majority class in its community\n",
    "    y_pred = np.zeros_like(y_true)\n",
    "    \n",
    "    for comm_idx, community in enumerate(communities_list):\n",
    "        # Get labels of nodes in this community\n",
    "        comm_labels = [G.get_node_data(node)['label'] for node in community]\n",
    "        \n",
    "        # Determine majority class\n",
    "        if len(comm_labels) > 0:\n",
    "            majority_label = 1 if sum(comm_labels) / len(comm_labels) >= 0.5 else 0\n",
    "            \n",
    "            # Assign majority label to all nodes in this community\n",
    "            for node in community:\n",
    "                y_pred[node] = majority_label\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics['precision'] = precision_score(y_true, y_pred, zero_division=0)\n",
    "    metrics['recall'] = recall_score(y_true, y_pred, zero_division=0)\n",
    "    metrics['f1'] = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Calculate AUC if possible\n",
    "    try:\n",
    "        metrics['auc'] = roc_auc_score(y_true, y_pred)\n",
    "    except:\n",
    "        metrics['auc'] = 0.5  # Default for random classifier\n",
    "    \n",
    "    # Calculate cluster purity\n",
    "    purities = []\n",
    "    for community in communities_list:\n",
    "        if len(community) > 0:\n",
    "            comm_labels = [G.get_node_data(node)['label'] for node in community]\n",
    "            majority_count = max(sum(comm_labels), len(comm_labels) - sum(comm_labels))\n",
    "            purity = majority_count / len(comm_labels)\n",
    "            purities.append(purity)\n",
    "    \n",
    "    metrics['avg_purity'] = np.mean(purities) if purities else 0\n",
    "    \n",
    "    # Number of communities and sizes\n",
    "    metrics['num_communities'] = len(communities_list)\n",
    "    \n",
    "    community_sizes = [len(comm) for comm in communities_list]\n",
    "    metrics['avg_community_size'] = np.mean(community_sizes) if community_sizes else 0\n",
    "    \n",
    "    # Calculate attack concentration\n",
    "    attack_concentration = {}\n",
    "    for comm_idx, community in enumerate(communities_list):\n",
    "        attack_count = sum(G.get_node_data(node)['label'] for node in community)\n",
    "        attack_concentration[comm_idx] = attack_count / len(community) if community else 0\n",
    "    \n",
    "    # Identify communities with high attack concentration\n",
    "    high_attack_comms = [idx for idx, conc in attack_concentration.items() if conc >= 0.8]\n",
    "    metrics['num_attack_communities'] = len(high_attack_comms)\n",
    "    metrics['attack_communities_ratio'] = len(high_attack_comms) / len(communities_list) if communities_list else 0\n",
    "    \n",
    "    return metrics, y_pred, attack_concentration\n",
    "\n",
    "# Apply traditional community detection methods\n",
    "if 'G' in locals():\n",
    "    # Check if results file exists\n",
    "    results_file = os.path.join(RESULTS_DIR, 'community_detection_results.pkl')\n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'rb') as f:\n",
    "            all_results = pickle.load(f)\n",
    "        print(f\"Loaded results for {len(all_results)} methods\")\n",
    "    else:\n",
    "        # Run methods\n",
    "        all_results = {}\n",
    "        \n",
    "        # Louvain\n",
    "        print(\"\\nRunning Louvain algorithm...\")\n",
    "        start_time = time.time()\n",
    "        louvain_communities, _ = run_louvain(G)\n",
    "        louvain_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Found {len(set(louvain_communities.values()))} communities in {louvain_time:.2f} seconds\")\n",
    "        louvain_metrics, louvain_pred, louvain_attack_conc = evaluate_communities(G, louvain_communities)\n",
    "        print(f\"Metrics: Accuracy={louvain_metrics['accuracy']:.4f}, F1={louvain_metrics['f1']:.4f}\")\n",
    "        \n",
    "        all_results['louvain'] = {\n",
    "            'communities': louvain_communities,\n",
    "            'execution_time': louvain_time,\n",
    "            'metrics': louvain_metrics,\n",
    "            'predictions': louvain_pred,\n",
    "            'attack_concentrations': louvain_attack_conc\n",
    "        }\n",
    "        \n",
    "        # Visualize Louvain communities\n",
    "        visualize_communities(G, louvain_communities, title=\"Louvain Communities\")\n",
    "        \n",
    "        # Run a few other methods for comparison\n",
    "        # Label Propagation\n",
    "        print(\"\\nRunning Label Propagation algorithm...\")\n",
    "        start_time = time.time()\n",
    "        lp_communities, _ = run_label_propagation(G)\n",
    "        lp_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Found {len(set(lp_communities.values()))} communities in {lp_time:.2f} seconds\")\n",
    "        lp_metrics, lp_pred, lp_attack_conc = evaluate_communities(G, lp_communities)\n",
    "        print(f\"Metrics: Accuracy={lp_metrics['accuracy']:.4f}, F1={lp_metrics['f1']:.4f}\")\n",
    "        \n",
    "        all_results['label_propagation'] = {\n",
    "            'communities': lp_communities,\n",
    "            'execution_time': lp_time,\n",
    "            'metrics': lp_metrics,\n",
    "            'predictions': lp_pred,\n",
    "            'attack_concentrations': lp_attack_conc\n",
    "        }\n",
    "        \n",
    "        # Save results\n",
    "        with open(results_file, 'wb') as f:\n",
    "            pickle.dump(all_results, f)\n",
    "    \n",
    "    # Display summary of results\n",
    "    print(\"\\nCommunity Detection Results Summary:\")\n",
    "    rows = []\n",
    "    for method_name, result in all_results.items():\n",
    "        metrics = result['metrics']\n",
    "        rows.append({\n",
    "            'Method': method_name,\n",
    "            'Num Communities': metrics['num_communities'],\n",
    "            'Avg Purity': metrics['avg_purity'],\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'], \n",
    "            'F1': metrics['f1'],\n",
    "            'Attack Comm. Ratio': metrics.get('attack_communities_ratio', 0),\n",
    "            'Execution Time (s)': result['execution_time']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Plot metrics comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics_df = summary_df.melt(\n",
    "        id_vars=['Method'],\n",
    "        value_vars=['Accuracy', 'Precision', 'Recall', 'F1', 'Avg Purity'],\n",
    "        var_name='Metric',\n",
    "        value_name='Value'\n",
    "    )\n",
    "    sns.barplot(x='Method', y='Value', hue='Metric', data=metrics_df)\n",
    "    plt.title('Performance Metrics by Method')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Attack-Related Communities\n",
    "\n",
    "Let's analyze the communities to identify those with high concentrations of attack traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attack_communities(G, communities, attack_concentrations, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Analyze communities with high concentrations of attack traffic\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G: rustworkx.PyGraph\n",
    "        Graph with features and labels\n",
    "    communities: dict\n",
    "        Community assignments\n",
    "    attack_concentrations: dict\n",
    "        Dictionary mapping community ID to attack concentration\n",
    "    threshold: float\n",
    "        Threshold for considering a community attack-focused\n",
    "    \"\"\"\n",
    "    # Identify communities with high attack concentration\n",
    "    attack_comms = {comm_id: conc for comm_id, conc in attack_concentrations.items() \n",
    "                   if conc >= threshold}\n",
    "    \n",
    "    if not attack_comms:\n",
    "        print(f\"No communities with attack concentration >= {threshold} found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(attack_comms)} communities with attack concentration >= {threshold}:\")\n",
    "    for comm_id, conc in sorted(attack_comms.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  Community {comm_id}: {conc:.2f} attack concentration\")\n",
    "    \n",
    "    # Convert community dict to lists\n",
    "    community_assignments = {}\n",
    "    for node, community in communities.items():\n",
    "        if community not in community_assignments:\n",
    "            community_assignments[community] = []\n",
    "        community_assignments[community].append(node)\n",
    "    \n",
    "    # Analyze characteristics of attack communities\n",
    "    attack_community_stats = []\n",
    "    for comm_id in attack_comms.keys():\n",
    "        if comm_id in community_assignments:\n",
    "            comm_nodes = community_assignments[comm_id]\n",
    "            \n",
    "            # Get node features for this community\n",
    "            features = []\n",
    "            for node in comm_nodes:\n",
    "                node_data = G.get_node_data(node)\n",
    "                if 'features' in node_data and isinstance(node_data['features'], (list, np.ndarray)):\n",
    "                    features.append(node_data['features'])\n",
    "            \n",
    "            if features:\n",
    "                features = np.array(features)\n",
    "                # Calculate mean and std of features\n",
    "                mean_features = np.mean(features, axis=0)\n",
    "                std_features = np.std(features, axis=0)\n",
    "                \n",
    "                attack_community_stats.append({\n",
    "                    'community_id': comm_id,\n",
    "                    'attack_concentration': attack_comms[comm_id],\n",
    "                    'size': len(comm_nodes),\n",
    "                    'mean_features': mean_features,\n",
    "                    'std_features': std_features\n",
    "                })\n",
    "    \n",
    "    if attack_community_stats:\n",
    "        # Visualize feature profiles of attack communities\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        for i, stats in enumerate(attack_community_stats[:5]):  # Show top 5 communities\n",
    "            plt.bar(\n",
    "                np.arange(len(stats['mean_features'])) + i * 0.15, \n",
    "                stats['mean_features'], \n",
    "                width=0.15, \n",
    "                yerr=stats['std_features'],\n",
    "                alpha=0.7,\n",
    "                label=f\"Comm {stats['community_id']} (Atk={stats['attack_concentration']:.2f})\"\n",
    "            )\n",
    "        \n",
    "        plt.xlabel('Feature Index')\n",
    "        plt.ylabel('Feature Value')\n",
    "        plt.title('Feature Profiles of High-Attack Communities')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Analyze attack communities for the best performing method\n",
    "if 'all_results' in locals() and all_results:\n",
    "    # Find the method with the highest F1 score\n",
    "    best_method = max(all_results.items(), key=lambda x: x[1]['metrics']['f1'])\n",
    "    print(f\"\\nAnalyzing attack communities for the best performing method: {best_method[0]}\")\n",
    "    \n",
    "    analyze_attack_communities(\n",
    "        G, \n",
    "        best_method[1]['communities'], \n",
    "        best_method[1]['attack_concentrations']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Performance Comparison\n",
    "\n",
    "In this notebook, we've applied community detection methods to the UNSW-NB15 dataset for cybersecurity analysis. Here's a summary of our findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a comprehensive summary of all methods\n",
    "if 'all_results' in locals() and all_results:\n",
    "    # Create a summary table\n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Display key findings\n",
    "    print(\"Key Findings:\")\n",
    "    \n",
    "    # Best method for accuracy\n",
    "    best_accuracy = summary_df.loc[summary_df['Accuracy'].idxmax()]\n",
    "    print(f\"1. Best method for accuracy: {best_accuracy['Method']} ({best_accuracy['Accuracy']:.4f})\")\n",
    "    \n",
    "    # Best method for F1 score\n",
    "    best_f1 = summary_df.loc[summary_df['F1'].idxmax()]\n",
    "    print(f\"2. Best method for F1 score: {best_f1['Method']} ({best_f1['F1']:.4f})\")\n",
    "    \n",
    "    # Most efficient method\n",
    "    most_efficient = summary_df.loc[summary_df['Execution Time (s)'].idxmin()]\n",
    "    print(f\"3. Most efficient method: {most_efficient['Method']} ({most_efficient['Execution Time (s)']:.4f}s)\")\n",
    "    \n",
    "    # Method with highest community purity\n",
    "    best_purity = summary_df.loc[summary_df['Avg Purity'].idxmax()]\n",
    "    print(f\"4. Method with highest community purity: {best_purity['Method']} ({best_purity['Avg Purity']:.4f})\")\n",
    "    \n",
    "    # Method with best attack community detection\n",
    "    best_attack_ratio = summary_df.loc[summary_df['Attack Comm. Ratio'].idxmax()]\n",
    "    print(f\"5. Method best at isolating attack traffic: {best_attack_ratio['Method']} ({best_attack_ratio['Attack Comm. Ratio']:.4f})\")\n",
    "    \n",
    "    # Create final comparison plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Scatter plot of F1 vs Execution Time\n",
    "    plt.scatter(\n",
    "        summary_df['Execution Time (s)'], \n",
    "        summary_df['F1'], \n",
    "        s=summary_df['Num Communities'] * 20,  # Size based on number of communities\n",
    "        alpha=0.7\n",
    "    )\n",
    "    \n",
    "    # Label each point with method name\n",
    "    for i, row in summary_df.iterrows():\n",
    "        plt.annotate(\n",
    "            row['Method'], \n",
    "            (row['Execution Time (s)'], row['F1']),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points'\n",
    "        )\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Execution Time (s)')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Community Detection Method Performance')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, 'performance_comparison.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nConclusion:\")\n",
    "    print(\"Community detection methods can effectively identify network traffic patterns related to cyber attacks.\")\n",
    "    print(f\"The {best_f1['Method']} method provided the best balance of precision and recall for identifying attack traffic.\")\n",
    "    print(f\"The {best_purity['Method']} method created the most homogeneous communities in terms of traffic type.\")\n",
    "    print(\"These methods can be valuable tools for network security monitoring and anomaly detection.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
